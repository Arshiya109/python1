{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMxHzT6tx0DqjCNaEtcQeNv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arshiya109/python1/blob/main/Module_01_Lab_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NAME: ARSHIYA MUBARAKALI SAIYYAD\n"
      ],
      "metadata": {
        "id": "WwHFcFFADAHU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2ta73LTw04T",
        "outputId": "97e1949e-7cc4-45ff-e12e-0515ef5c1e6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-05cb118fe4a9>:10: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  dataset.target = dataset.target.astype(np.int) # so that we can classify\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20640, 8)\n",
            "(20640,)\n",
            "Number of test samples =  4144\n",
            "Number of other samples =  16496\n",
            "Percent of test data =  20.07751937984496 %\n",
            "Train accuracy using nearest neighbour is  1.0\n",
            "Train accuracy using random classifier is  0.164375808538163\n",
            "Validation accuracy using nearest neighbour is  0.34108527131782945\n",
            "Validation accuracy using random classifier is  0.1688468992248062\n",
            "Validation accuracy of nearest neighbour is  0.34048257372654156\n",
            "Test accuracy is  0.34917953667953666\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "# set randomseed\n",
        "rng = np.random.default_rng(seed=42)\n",
        "\n",
        "dataset =  datasets.fetch_california_housing()\n",
        "# print(dataset.DESCR)  # uncomment this if you want to know more about this dataset\n",
        "# print(dataset.keys())  # if you want to know what else is there in this dataset\n",
        "dataset.target = dataset.target.astype(np.int) # so that we can classify\n",
        "print(dataset.data.shape)\n",
        "print(dataset.target.shape)\n",
        "\n",
        "def NN1(traindata, trainlabel, query):\n",
        "  diff  = traindata - query  # find the difference between features. Numpy automatically takes care of the size here\n",
        "  sq = diff*diff # square the differences\n",
        "  dist = sq.sum(1) # add up the squares\n",
        "  label = trainlabel[np.argmin(dist)] # our predicted label is the label of the training data which has the least distance from the query\n",
        "  return label\n",
        "\n",
        "def NN(traindata, trainlabel, testdata):\n",
        "  # we will run nearest neighbour for each sample in the test data\n",
        "  # and collect the predicted classes in an array using list comprehension\n",
        "  predlabel = np.array([NN1(traindata, trainlabel, i) for i in testdata])\n",
        "  return predlabel\n",
        "\n",
        "def RandomClassifier(traindata, trainlabel, testdata):\n",
        "  # in reality, we don't need these arguments\n",
        "\n",
        "  classes = np.unique(trainlabel)\n",
        "  rints = rng.integers(low=0, high=len(classes), size=len(testdata))\n",
        "  predlabel = classes[rints]\n",
        "  return predlabel\n",
        "\n",
        "def Accuracy(gtlabel, predlabel):\n",
        "  assert len(gtlabel)==len(predlabel), \"Length of the groundtruth labels and predicted labels should be the same\"\n",
        "  correct = (gtlabel==predlabel).sum() # count the number of times the groundtruth label is equal to the predicted label.\n",
        "  return correct/len(gtlabel)\n",
        "\n",
        "def split(data, label, percent):\n",
        "  # generate a random number for each sample\n",
        "  rnd = rng.random(len(label))\n",
        "  split1 = rnd<percent\n",
        "  split2 = rnd>=percent\n",
        "  split1data = data[split1,:]\n",
        "  split1label = label[split1]\n",
        "  split2data = data[split2,:]\n",
        "  split2label = label[split2]\n",
        "  return split1data, split1label, split2data, split2label\n",
        "\n",
        "testdata, testlabel, alltraindata, alltrainlabel = split(dataset.data, dataset.target, 20/100)\n",
        "print('Number of test samples = ', len(testlabel))\n",
        "print('Number of other samples = ', len(alltrainlabel))\n",
        "print('Percent of test data = ', len(testlabel)*100/len(dataset.target),'%')\n",
        "\n",
        "traindata, trainlabel, valdata, vallabel = split(alltraindata, alltrainlabel, 75/100)\n",
        "\n",
        "trainpred = NN(traindata, trainlabel, traindata)\n",
        "trainAccuracy = Accuracy(trainlabel, trainpred)\n",
        "print(\"Train accuracy using nearest neighbour is \", trainAccuracy)\n",
        "\n",
        "trainpred = RandomClassifier(traindata, trainlabel, traindata)\n",
        "trainAccuracy = Accuracy(trainlabel, trainpred)\n",
        "print(\"Train accuracy using random classifier is \", trainAccuracy)\n",
        "\n",
        "valpred = NN(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy using nearest neighbour is \", valAccuracy)\n",
        "\n",
        "valpred = RandomClassifier(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy using random classifier is \", valAccuracy)\n",
        "\n",
        "traindata, trainlabel, valdata, vallabel = split(alltraindata, alltrainlabel, 75/100)\n",
        "valpred = NN(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy of nearest neighbour is \", valAccuracy)\n",
        "\n",
        "testpred = NN(alltraindata, alltrainlabel, testdata)\n",
        "testAccuracy = Accuracy(testlabel, testpred)\n",
        "print('Test accuracy is ', testAccuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.  **How is the accuracy of the validation set affected if we increase the percentage of validation set? What happens when we reduce it?**\n",
        "\n",
        "\n",
        "*   When we increase the percentage of the validation set (e.g., by reducing the size of the training set), we allocate more data for validation and less for training.\n",
        "\n",
        "*  As it is affected into the two ways as:- a)Positive Effect on Validation Accuracy and b)Negative Effect on Training Accuracy:\n",
        "     \n",
        "*  **Positive Effect on Validation Accuracy:-**With a larger validation set, the validation accuracy tends to be a more reliable estimate of how well the model generalizes to unseen data. It can better capture the generalization performance of the model because it's evaluated on a larger representative sample of data.\n",
        "\n",
        "*  **Negative Effect on Training Accuracy:-**However, the training accuracy may decrease as the training set size decreases. The model has less data to learn from, which can lead to underfitting. It may not capture complex patterns in the data as effectively.\n",
        "\n",
        "*  If we reduce the percentage of validation set,then we allocate more data for training and less for validation.\n",
        "\n",
        "*  As it will affect into two ways as:-a)Positive Effect on Training Accuracy and b)Negative Effect on Validation Accuracy\n",
        "\n",
        "*  **Positive Effect on Training Accuracy:-** With a smaller validation set, the training accuracy tends to increase because the model has more data to learn from. It can capture complex patterns in the data and may perform better on the training set.\n",
        "\n",
        "*  **Negative Effect on Validation Accuracy:-** However, the validation accuracy may become less reliable. The smaller validation set may not effectively represent the variability in the data, making it less useful for estimating generalization performance. It might not detect overfitting as effectively because the model has more training data"
      ],
      "metadata": {
        "id": "g1kjROeH0W8P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **How does the size of the train and validation set affect how well we can predict the accuracy on the test set using the validation set?**\n",
        "\n",
        "\n",
        "*  The size of the training and validation sets can affect how well we can predict the accuracy on the test set using the validation set in the following ways, such as:\n",
        "\n",
        "*  **If the Training Set is larger:** A larger training set typically allows the model to learn more about the underlying patterns in the data. This can result in a model that generalizes better to unseen data, including the validation and test sets.With a larger training set, the model is likely to perform well on both the training and validation sets. This can lead to a more optimistic estimate of the model's performance. In other words, if the model is overfitting the training data, a larger training set may make it less apparent during validation.\n",
        "\n",
        "*   **If the Validation Set is larger:-** A larger validation set provides a more reliable estimate of the model's performance on unseen data. It reduces the variability in validation scores and helps in detecting overfitting.With a larger validation set, the validation accuracy tends to be a better indicator of how well the model will perform on the test set. If the model performs well on a larger and more representative validation set, it is more likely to generalize well to the test set.\n",
        "\n",
        "*  **If the Training Set is smaller:-** A smaller training set might still allow the model to learn essential patterns while preventing overfitting. It can be beneficial when dealing with limited data.However, if the training set is too small, the model may not capture complex patterns effectively, leading to underfitting. In such cases, the validation accuracy may not accurately predict the test accuracy because the model hasn't learned enough from the training data.\n",
        "\n",
        "*   **If the Validation Set is smaller:-** A smaller validation set can be useful when you have limited data, as it allows you to allocate more samples for training. It can also be useful when you want to train multiple models with different hyperparameters.However, a smaller validation set may provide a less reliable estimate of model performance. It may not capture the variability in the data, making it less effective at predicting how well the model will perform on the test set."
      ],
      "metadata": {
        "id": "3kvVLpuz4wK-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **What do you think is a good percentage to reserve for the validation set so that thest two factors are balanced?**\n",
        "\n",
        "\n",
        "\n",
        "*  The choice of the percentage to reserve for the validation set depends on various factors, and there is no one-size-fits-all answer. However, a common practice in machine learning is to use a 70-30 or 80-20 split for training and validation, respectively. This means reserving 20% to 30% of your data for validation and using the remaining 70% to 80% for training.\n",
        "\n",
        "*   There are some considerations to help you decide on a good percentage for the validation set:\n",
        "\n",
        "*   Dataset Size: If you have a relatively small dataset, you may want to allocate a larger percentage to the validation set to ensure you have enough data to reliably estimate model performance. In such cases, a 70-30 or 80-20 split is reasonable.\n",
        "\n",
        "*   Complexity of the Model: If you are using a highly complex model with many parameters (e.g., deep neural networks), you may want to allocate a larger validation set. Complex models are more prone to overfitting, and a larger validation set can help in early detection of overfitting.\n",
        "\n",
        "*   Data Quality: If your dataset contains noisy or outlier-prone data, a larger validation set can help in obtaining a more robust estimate of model performance.\n",
        "\n",
        "*   Computational Resources: Consider the computational resources available to you. Training on a larger training set requires more computation. If resources are limited, you might opt for a smaller validation set.\n",
        "\n",
        "*   Cross-Validation: If you have a small dataset and want a more reliable estimate of model performance, you can use techniques like k-fold cross-validation. In k-fold cross-validation, you divide your data into k subsets and perform training and validation k times, using each subset as the validation set once. This allows you to use all your data for both training and validation, mitigating the need for a large fixed validation set.\n",
        "\n",
        "*  Iterative Experimentation: You can experiment with different validation set percentages. Start with a reasonable split (e.g., 70-30), and if you find that the model's performance is not stable or that you suspect overfitting, consider increasing the size of the validation set.\n",
        "\n",
        "*   Domain Knowledge: Consider any domain-specific knowledge or guidelines that might apply to your specific problem. Some domains may have established best practices for data splitting.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "glhflJaH8dBw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# you can use this function for random classifier also\n",
        "def AverageAccuracy(alldata, alllabel, splitpercent, iterations, classifier=NN):\n",
        "  accuracy = 0\n",
        "  for ii in range(iterations):\n",
        "    traindata, trainlabel, valdata, vallabel = split(alldata, alllabel, splitpercent)\n",
        "    valpred = classifier(traindata, trainlabel, valdata)\n",
        "    accuracy += Accuracy(vallabel, valpred)\n",
        "  return accuracy/iterations # average of all accuracies\n",
        "\n",
        "print('Average validation accuracy is ', AverageAccuracy(alltraindata, alltrainlabel, 75/100, 10, classifier=NN))\n",
        "testpred = NN(alltraindata, alltrainlabel, testdata)\n",
        "print('test accuracy is ',Accuracy(testlabel, testpred) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVBtgHgu-fcr",
        "outputId": "2acfe284-30a8-4bbe-8fcf-8fc0d9ac3c6e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average validation accuracy is  0.33584635395170215\n",
            "test accuracy is  0.34917953667953666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   **Does averaging the validation accuracy across multiple splits give more consistent results?**\n",
        "\n",
        "* Yes, averaging the validation accuracy across multiple splits does give more consistent and robust results compared to relying on a single train-validation split. Here's why averaging validation accuracy is beneficial:\n",
        "\n",
        "* Reducing Variability: Machine learning models can be sensitive to the particular random split of the data into training and validation sets. A single split may lead to variability in the observed validation accuracy, which may not be representative of the model's true performance.\n",
        "\n",
        "* Mitigating Overfitting: By repeatedly splitting the data and calculating validation accuracy, you get a better sense of how well the model generalizes to different subsets of the data. If your model is overfitting, it's more likely to be detected when you average the results over multiple splits.\n",
        "\n",
        "* Robustness: Averaging helps in obtaining a more stable and robust estimate of the model's performance. It reduces the impact of outliers or unusual data splits that can lead to misleading results in a single split.\n",
        "\n",
        "* Better Reflecting Real-World Performance: In practice, your model will encounter different distributions of data over time. Averaging validation accuracy simulates this variability and provides a more realistic estimate of how the model will perform in a production setting.\n",
        "\n",
        "*   AverageAccuracy provides an estimate of the model's validation performance over multiple splits, which can be helpful for assessing how well your model generalizes.\n",
        "\n",
        "* The test accuracy is computed separately using the NN classifier on the test data.\n",
        "\n",
        "* The combination of the average validation accuracy and the test accuracy gives you a sense of how well your model is likely to perform on unseen data. The average validation accuracy helps assess how well your model is expected to perform on data similar to your training data but not seen during training, and the test accuracy provides an evaluation of the model's performance on completely new, unseen data."
      ],
      "metadata": {
        "id": "O_Ywoz9i-JAM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Does it give more accurate estimate of test accuracy?**\n",
        "\n",
        "*   Averaging validation accuracy across multiple splits does not directly provide a more accurate estimate of test accuracy. The purpose of averaging validation accuracy is to obtain a more stable and reliable estimate of how well your model generalizes to unseen data (i.e., validation data) within your dataset. It helps you assess the model's performance on data that it hasn't seen during training, providing a better indication of its ability to generalize.\n",
        "\n",
        "* The estimate of test accuracy, on the other hand, is based on the model's performance on a completely separate dataset that it has never encountered during training or validation. Test accuracy provides a measure of how well the model performs on new, previously unseen data, which is a crucial indicator of its real-world performance.\n",
        "\n",
        "* While averaging validation accuracy helps you make more informed decisions about your model's generalization performance within your dataset, it doesn't directly predict the test accuracy. The test accuracy is an independent evaluation metric that reflects how well your model performs on completely new and different data.\n"
      ],
      "metadata": {
        "id": "2sIfVxIq_8iW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **What is the effect of the number of iterations on the estimate? Do we get a better estimate with higher iterations?**\n",
        "\n",
        "\n",
        "*   The number of iterations in techniques like cross-validation or averaging validation accuracy does have an effect on the estimate of model performance. Generally, increasing the number of iterations can lead to a more reliable and stable estimate of model performance, up to a point. There is how the number of iterations affects the estimate:\n",
        "\n",
        "* Fewer Iterations: With fewer iterations, you get a quicker estimate of model performance. It's computationally less expensive.The estimate might be less stable and more sensitive to the specific random splits of the data. You might not capture the full variability in the data.\n",
        "\n",
        "* More Iterations: Increasing the number of iterations helps in obtaining a more robust estimate of model performance. It averages out the effects of different data splits and provides a more stable estimate.It can be computationally more intensive and time-consuming, especially if the model training process is resource-intensive.\n",
        "\n",
        "* Diminishing Returns: While increasing the number of iterations tends to improve the estimate, there are diminishing returns. Beyond a certain point, adding more iterations might not significantly improve the estimate's stability. The improvement becomes marginal, and the computational cost becomes the dominant factor.\n",
        "\n",
        "* More iterations tend to provide a better estimate of model performance, but the choice of the number of iterations should balance computational cost and the need for a stable estimate. The specific number of iterations will depend on your dataset, modeling goals, and available resources.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VNfkXGWnBCoW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.  **Consider the results you got for the previous questions. Can we deal with a very small train dataset or validation dataset by increasing the iterations?**\n",
        "\n",
        "\n",
        "*   Increasing the number of iterations can help mitigate some of the issues associated with having very small training or validation datasets, but it doesn't fully address the fundamental limitations of small datasets. there is how increasing iterations can help when dealing with small datasets:\n",
        "\n",
        "* Increased Robustness: When you have a small training or validation dataset, the results can be highly sensitive to the specific data split. Increasing the number of iterations allows you to perform multiple splits and average the results, which can provide a more robust estimate of model performance. This averaging helps reduce the impact of random variations in the data.\n",
        "\n",
        "* Better Detection of Overfitting: With small datasets, models are more prone to overfitting because they can potentially memorize the training data. By using multiple iterations and different data splits, you increase the chances of detecting overfitting. If the model consistently performs well across different splits, it's less likely to be overfitting.\n",
        "\n",
        "* Stability in Validation: Averaging validation accuracy across multiple iterations can make the estimate of model performance more stable. This stability is particularly important when dealing with small validation datasets because a single unusual split can lead to misleading results.\n",
        "\n",
        "* However, it's essential to note the limitations:\n",
        "\n",
        "* Data Quantity: Increasing iterations doesn't increase the amount of data available for training or validation. It can improve the reliability of your performance estimate, but it doesn't provide more data to the model for learning. Small datasets may still limit the model's ability to capture complex patterns.\n",
        "\n",
        "* Computational Cost: Running a large number of iterations can be computationally expensive, especially if the model training process is resource-intensive. You should consider the trade-off between computational cost and the improvement in performance estimate.\n",
        "\n",
        "* Generalization to Unseen Data: While increasing iterations helps assess model stability and reliability within your dataset, it doesn't address the primary goal of machine learning: to build models that generalize well to unseen data. Small datasets may still result in models that struggle to generalize to new, unseen examples.\n",
        "\n",
        "* Increasing the number of iterations is a valuable technique to obtain more stable and reliable estimates of model performance, especially when dealing with small datasets. It helps mitigate some of the challenges associated with data scarcity, but it doesn't fully replace the need for larger datasets to improve model generalization to unseen data.\n"
      ],
      "metadata": {
        "id": "dGkS-0VUB6C6"
      }
    }
  ]
}